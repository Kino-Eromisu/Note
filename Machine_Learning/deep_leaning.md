# 深度学习
## 神经网络(怎么看不懂)
神经网络其实就是一个输入X到输出Y的映射函数：f(X)=Y，函数的系数就是我们所要训练的网络参数W  
### 前向传播
### 反向传播
反向传播算法的核心是代价函数 C 对网络中参数（各层的权重w和偏置b）的偏导表达式 ∂C/∂w 和 ∂C/∂b。这些表达式描述了代价函数值C随权重w或偏置b变化而变化的程度。到这里，BP算法的思路就很容易理解了：如果当前代价函数值距离预期值较远，那么我们通过调整w和b的值使新的代价函数值更接近预期值（和预期值相差越大，则w和b调整的幅度就越大）。一直重复该过程，直到最终的代价函数值在误差范围内，则算法停止。

## 基础知识
### 激活函数
帮网络引入“非线性”。能使神经网络学会模式和规律。否则线性关系的神经网络实在太差。  
CNN常用激活函数：
1. Sigmoid
  - 二分类，分为0和1
  - 可能导致梯度消失
2. Relu
  - 小于0输出0，大于0输出不变
  - 缓解梯度消失（函数导数：x<0 时恒等于0，大于等于0时为1，也就是说此时Relu的梯度恒等1，不会变小不会消失。反之出现神经元失活）
  - 在深层网络中仍可能出现梯度爆炸（因连续放大）
  - **GELU似乎更常用一些，不确定**
3. Tanh
  - 把数字归一到-1到1之间
4. Leaky Relu
  - 负数区域允许小斜率通过，保留一点点漏洞α，让网络在负区间也能学习，避免信息完全丢失
  - 改善ReLU“死亡神经元”问题（死亡神经元：输入为负，梯度恒等于0，永远无法更新）
  - **需要手动调参，使用率低，一般使用Relu+残差连接，但也可以直接使用默认的参数**
5. Softmax
  - 多分类

Transformer常用激活函数：
Transformer中可能分为不同部分的激活函数：
- 前馈网络FFN中，常用Relu、Gelu
  - GELU
    - GELU(x) = x * Φ(x) Φ为标准正态分布的累积分布函数
    - 类似Leaky Relu，但更柔和
    - 允许少量负值通过
    - **使用率高，常用于NLP、深层模型。自适应性更强（通过概率建模动态处理负数区）**
  - Swish（x * sigmoid(βx)）
    - 优于Relu，但计算量略高
- 自注意力机制
  - Softmax
    - 突出重要特征（放大最大值），抑制无关信息。
    - 梯度友好，但可能引发梯度消失（极端分布时）。
- 输出层
  - 分类：Softmax、Sigmoid
  - 回归：线性输出，没有激活函数

### BN 批量归一化
全称：Batch Normalization  
可以通过规整数据的分布基本解决梯度消失/爆炸的问题（避免输入值过大或过小），但在RNN或是极深的网络中不能完全解决，还是需要激活函数、残差连接等。  
简单来讲，就是使神经网络中的数据保持稳定。  
神经网络在训练时，每一层输入的数据分布会不断变化（叫“内部协变量偏移”），导致训练变慢、不稳定。BN把每一层输入的数据“调成”**均值为0、方差为1**的标准状态，自动调整每一层的输入，让网络训练更快更稳定。

1. 把当前 mini-batch （一小批训练样本）里的数据算均值和方差。
2. 用这个均值和方差把数据“归一化”，变成平均值为0，标准差为1。
3. 加上两个可学习的参数（缩放因子γ和平移因子β），让网络自己决定是否保留归一化结果或者做调整。

一般在Relu激活函数的前面使用。

### LN 层归一化
与BN区别：BN是对同一通道中的所有图片进行处理，LN是对一张图片中的所有通道进行处理（同时）  
BN跨Batch归一化通道、LN跨Channel归一化样本
图像数据（4D张量）
- B（Batch）批次中的样本数量
- C（Channel）数据通道数
- W（Width）宽度
- H（Height）高度
序列数据（3D张量）
- B（Batch）
- L（Length）序列长度，如单词数
- D（Dimension）特征维度，如词向量的长度

## 常见架构
- CNN（以下均为局部感受野（卷积）作为核心特征提取手段，且均为浅层到深层）
  - ResNet
  - Yolo
  -（V-Net）
- Transformer
  - 基于自注意力（Self-Attention）机制
  - BERT
    - Google的一个自然语言处理模型
    - 基于transformer encoder构建，但没用decoder，因为BERT是用来做问答、分类等理解类任务的，而不是翻译、写作等生成任务
    - 使用多头注意力机制，使用残差连接和layernorm，使用前馈神经网络
  - GPT
    - 只使用了decoder的堆叠部分，没完整使用decoder，没用encoder
    - 用自回归方式进行文本生成
  - ViT
    - 视觉图像专用transformer
- RNN
- MLP  

## CNN
核心：卷积、池化  
擅长做图像、网格数据处理  
复杂度：O(n)局部感受野  
部分并行（依赖卷积核大小）	
**核心精髓： 利用局部连接、参数共享、空间层次化（池化） 高效学习图像的平移不变特征。**
可以分为卷积层、池化层、全连接层
卷积层特性：
- 分层表示特征，浅层中层深层不同程度
- 空间保持性：每个位置对应输入图像的局部区域（感受野）
- 多通道机制：每个卷积核生成一个特征通道，通道间组合表示复杂特征（如颜色+纹理+形状）
- 局部连接：只连接前一层的局部区域，只处理局部像素内，比如3x3，5x5
- 参数共享：同一卷积层的所有神经元（卷积核）使用相同的权重（filter）
卷积过程如图所示：  <img width="583" height="460" alt="image" src="https://github.com/user-attachments/assets/aa94b48f-c4a2-43e9-8a90-d311f254fb26" />

从数学的层次来讲，卷积就是将卷积核大小内的像素点，经过加权和，得到一个新的像素点。这个像素点在图像的层次上就可以反映原图像的特征。上图中原图像对应卷积的位置大小称为得到像素点对应的感受野（Receptive Field)。随着卷积核的平移，原图像的像素点会通过卷积得到一张新的特征图（也就是上图右边的那张），经过卷积后，输出的特征图可有效提取原图像的特征，达到图像识别的功能。

个人理解总结：  
可以把整张图像的卷积输出的特征想象成特征池（标准来讲是多维特征图），突出的显著特征会高一些。卷积操作通过滑动窗口（卷积核）对图像进行局部扫描，提取不同区域的空间特征。对卷积得到的特征使用激活函数进行激活，可以解决非线性问题。（由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。）对特征池中的所有特征进行池化，缩小池中的信息，保留最显著的特征。池子可能不止一个，有浅层和深层，浅层可能为边缘，深层可能为物体检测。所以浅层是对图像进行初步的边缘、颜色、纹理，深层是学习如何组合浅层输出的特征，如语义。  
**实际代码中，这些“池子”就是多维张量（如[batch, channels, height, width]）**

### Yolo 目标检测 2D
Backbone (CNN) → Neck (FPN/PAN) → Head (分类+回归)  
实时的目标检测算法  
YOLO 会将整个图像划分为一个 S×S 网格，每个网格负责预测一定数量的目标（v1-v5），或者像素点中心（v8）
- 坐标（x, y, w, h）
- 置信度
- 类别概率向量:“在当前位置有目标”的前提下，该目标属于每个类别的概率
最终输出是一个形如 [N, 6] 的张量，N 为检测到的目标数，6 包括 [x, y, w, h, confidence, class]
YOLO 的输出是密集预测的框集合，它并不会尝试把多个相邻网格的目标“拼接”起来。最后会通过非极大值抑制来筛选掉重复的检测框，留下可能性最高，最可靠的检测框。

检测过程：
1. 输入一张猫猫图，640x640x3（RGB）
2. backbone层，CNN，卷它，拿特征
3. 然后通过激活函数，使它非线性化。通过池化层，对特征进行降采样。
4. 现在它是一个有明显特征的小尺寸的复杂通道猫猫（80x80x256），此时可能输出：高分辨率猫耳朵、中分辨率猫脸、低分辨率整只猫
5. Neck层，v8中使用PAN，进行多尺度特征融合。让网络同时关注大目标（大区域）和小目标（细节）。具体来讲，backbone层分别学习了局部和全局，但互相不知道。PAN层的意义类似于交流情报，让高语义的输出结果给小目标，让它知道这是猫。把小目标的精度输出，增强细节。将情报（多层特征图）融合后，最终每一层都具备了语义+细节。
6. HEAD输出预测，每个位置都做一个检测框。假设我现在是（80x80），那就有6400个位置，每个位置可以预测多个检测框：[x, y, w, h, objectness, class_probs...]（坐标，置信度，类别概率）**YOLOV8使用Anchor-Free，这里的框可以由网络直接预测中心点、偏移量、宽高，不需要手动定义**
7. 后处理，非极大值抑制NMS。在众多猫猫框中，选出最猫猫的那个：筛选掉低分数的猫猫框，将框和分数最高的那个一一做对比（有一个阈值），小于的放一边（放下一轮去看，万一不止一只猫猫），大于阈值的，就是框重叠了，合并成分数最高的那个


### ResNet(2015) 残差神经网络 2D
x → [Conv1x1, Conv3x3(groups=32), Conv1x1] + x → ReLU  
在ResNet网络提出之前，传统的卷积神经网络都是通过将一系列卷积层与下采样层进行堆叠得到的。但是当堆叠到一定网络深度时，就会出现两个问题：
- 梯度消失或梯度爆炸
  - 梯度消失/爆炸：神经网络在**反向传播**的时候，反向连乘的梯度小于1（或大于1），导致连乘的次数多了之后（网络层数加深），传回首层的梯度过小甚至为0（过大甚至无穷大）
- 退化问题(degradation problem)
  - 网络越深，优化越困难，导致训练误差上升（即退化。不属于过拟合，而是模型本身难以学习。
- 其它扩展随便写这里了，不属于这里：过拟合
  - 过拟合：网络在训练集上表现很好，在验证集上表现差

梯度爆炸可以通过BN来解决，ResNet可以解决网络的退化问题。

普通的神经网络如下：
<img width="232" height="429" alt="image" src="https://github.com/user-attachments/assets/9deac40b-d788-476a-8d9f-86414ba05fd6" />  
残差神经网络如下:
<img width="1189" height="650" alt="image" src="https://github.com/user-attachments/assets/ac23bd27-dcab-4c35-819e-3beb7ee90bf0" />  
可以简单理解为：传统的神经网络是构建了一个完整的函数，这种方法会造成信息丢书。而残差神经网络是只学习差值，更加稳定可控。学习的是 F(x)，不是 H(x)。
H(x)对于模型来说可以理解为从头学起，全部依靠网络学习。而F(x)可以理解为模型的默认输出就是x，F(x)只是一个微调的值。


### V-Net 医学分割 3D
U-Net 2D: [Downsample → 2D Conv] ×4 → [Upsample + Skip] ×4  
V-Net 3D: [Downsample → 3D Conv] ×4 → [Upsample + 3D Skip] ×4  
编码器（下采样）  
1. 使用多个3D卷积 + Relu。
2. 使用stride=2的卷积代替了最大池化的下采样。stride=2 的卷积会让特征图的尺寸变为原来的一半（在每个空间维度上），类似于 max pooling 的效果，但带有参数（可学习）。
3. 提取低分辨率高语义的体积特征（利用深度信息，学习的还是语义）
解码器（上采样）
1. 使用转置卷积进行上采样（deconv）
2. 每一步 upsample 都和对称的 encoder 层拼接（skip connection），类似ResNet

特殊：使用 Dice 损失函数（Dice Loss），比交叉熵更适合处理类别不平衡问题，提升小目标表现


### CNN架构对比表（YOLO vs ResNet vs V-Net）

| 对比维度         | YOLO (以YOLOv8为例)                          | ResNet (以ResNet50为例)                  | V-Net                              |
|------------------|---------------------------------------------|-----------------------------------------|-----------------------------------|
| **核心结构**     | CSPDarknet + PANet + 检测头                 | 残差块 (Bottleneck) + 全局平均池化       | 3D U-Net结构 + 跳跃连接            |
| **输入维度**     | 2D RGB图像 (e.g., 640×640×3)                | 2D RGB图像 (e.g., 224×224×3)            | 3D体数据 (e.g., 128×128×128×1)    |
| **主要任务**     | 实时目标检测 (分类+定位)                    | 图像分类                                | 医学影像分割                      |
| **特征提取差异** | 多尺度特征融合 (FPN/PANet)                  | 深层语义特征 (通过残差连接保留梯度)       | 3D空间上下文特征 (体素级分析)      |
| **核心创新点**   | 1. 单阶段检测<br>2. Anchor-Free设计         | 1. 残差学习<br>2. 恒等映射              | 1. 3D卷积<br>2. Dice Loss         |
| **输出形式**     | 边界框 (x,y,w,h) + 类别概率                 | 类别概率向量                            | 3D分割掩码 (与输入同尺寸)          |
| **典型应用**     | 自动驾驶、视频监控                          | ImageNet分类、迁移学习                  | 肿瘤分割、器官三维重建             |
| **计算复杂度**   | 中 (约50 GFLOPs @640×640)                  | 高 (约4 GFLOPs @224×224)               | 极高 (约500 GFLOPs @128³)         |
| **开源实现**     | [Ultralytics](https://github.com/ultralytics) | [TorchVision](https://pytorch.org/vision) | [MONAI](https://monai.io/)       |  

## CNN扩展，并非不重要或不常用，只是后加入笔记中
### LeNet(1998)
Yan LeCun发明，应用于图像分类。主要用于**手写数字识别**
结构：卷积层 → 子采样层（平均池化） → 全连接层
激活函数使用sigmoid/tanh
输入灰度图

### AlexNet(2012)
用于ImageNet 分类（1000类） 
激活函数使用Relu  
Dropout：抑制全连接层过拟合  
数据增强：平移、翻转、变化颜色  
GPU 加速训练  
使用 大卷积核（11×11、5×5） 和更深层数  
LRN（Local Response Normalization）层（后来很少用了）

### VGG(2014)
用**更小的卷积核（3×3）**堆叠代替大卷积核  
两个 3×3 卷积相当于一个 5×5 卷积，但参数更少、非线性更多  
网络深度加大（VGG16、VGG19）  
结构非常规则：
卷积(3×3)×2/3 → 最大池化(2×2) → 重复

证明了深度和小卷积核的有效性  
结构规整，便于迁移到其他任务（分割、检测等）

### GoogleNet(2014)
Inception 模块：在同一层内用多种卷积核（1×1、3×3、5×5）+ 池化并行提取特征，然后在通道维度拼接  
1×1 卷积 做通道降维（减少计算量）  
网络深度更深，但参数量比 VGG 小很多  
引入辅助分类器（辅助梯度传播，减轻梯度消失）
  
## Transformer
核心：	自注意力、前馈网络
擅长：文本、序列数据，但现在也可用于图像  
O(n^2)全局注意力  
训练阶段可以完全并行，但在语言生成的推理阶段仍然为顺序执行  

Transformer（ViT）：将图像分块为序列，用Transformer处理，挑战CNN的视觉霸权  

可以将transformer理解为动态特征池。当输入图像时，图像会被拆解成一个个拼图块，排列成序列，其中每一个图像块转化为特征向量。使用自注意力机制，每个拼图块自主寻找其他相关块（如猫猫头找猫猫身），通过加权求和重构自身特征（找更多的猫猫特征）。重组后的拼图块被分类头解码为最终结果。（学习到图像中有猫猫）  

CNN的“池”是局部固定的（卷积核尺寸限制），靠深度堆叠扩大感受野。
Transformer的“池”是全局动态的，通过注意力权重直接关联任意距离的元素。 

1.  **Self-Attention 自注意力**：输入一个序列，输出一个序列，长度相同。每个位置都需要计算注意力，允许模型理解上下文关系。：QWE和EWQ
2.  **Multi-Head Attention 多头注意力**：拆分，分别进行学习：1关注“语义”，2关注“词性”，3关注“句法”
3.  **Cross-Attention 交叉注意力**：在序列中解码当前词，到另一个序列中编码输出：常用于翻译模型，输入ich，解码器输出ich，到编码器中查找我，输出翻译
4.  **Sparse Attention 稀疏注意力**：每个位置只关注部分重要位置，降低计算复杂度
5.  **Local Attention 局部注意力**：每个位置只关注临近的区域，如相邻四位
6.  **Global Attention 全局注意力**：每个位置对所有位置都关注，精度高，开销大

### Vit 图像分类
全称：Vision Transformer
1. 将图像分成指定大小的非重叠像素块，展平排列为向量序列
2. 每一个图像像素块都有一个可学习的位置信息，用来弥补对空间感知的缺陷
3. 通过自注意力机制建模图像中各区域之间的关系，最后输出图像分类或分割结果
4. 结果序列开头加一个 class

**大小一般为16x16、8x8、32x32、14x14**，16×16（性能与计算量的平衡点），资源受限：32×32（速度优先），高精度需求：8×8（需配合混合精度训练）。**ImageNet标准输入尺寸为224×224**

详细举例：
1. 输入图像 224x224x3（RGB、BGR无具体要求）
2. patch 大小：16x16
3. 得到 14x14 = 196 个 patch，加通道数一起算就是 16x16x3 = 768 维的向量序列
4. 加上位置信息，第一个 0，作为可学习的位置编码
5. 在序列前插入一个特殊向量[CLS]作为类别
6. 最终，输入序列长度变为197：196 patch + 1CLS。维度不变，768。整个序列的shape：[1,197,768]
7. 将整个patch序列喂给transformer编码器（比如BERT），编码器有多层，每一层编码器都做同样的事情：Multi-head Self-Attention → 残差连接 → LayerNorm → MLP → 残差连接 → LayerNorm
8. 在每一层的自注意力机制中，cls会获取其它patch的信息，模型会理解上下文的关系。
9. [CLS] token 的向量就成了一个图像全局语义的表达向量，它看过且学习了全图所有区域的信息

图像 → Patch → [CLS] + Patch序列 → Transformer 编码器 → 得到 [CLS] 向量  → 全连接层 → 分类 logits → softmax → 各类别概率

## CNN + Transformer = LETNet
CNN U-Net 在编码器输出的特征图中添加 Transformer 模块，让局部特征通过自注意力增强全局感受野  
CNN：强在局部卷积感受野、平移不变性  
Transformer：强在长距离依赖建模、捕获全局上下文  
LETNet 的设计思想：CNN 捕捉低层细节 + Transformer 捕捉高层全局关系，并合理降低内存消耗：  
<img width="458" height="368" alt="image" src="https://github.com/user-attachments/assets/d7903eb1-86af-4337-8bf6-7a9e34b598df" />
  
1. 论文中提出了一种Lightweight Dilated Bottleneck（LDB，轻量膨胀瓶颈结构），**由膨胀卷积和深度可分离卷积组成**，可极大压缩参数量和计算量.  <img width="431" height="358" alt="image" src="https://github.com/user-attachments/assets/3d6a6466-66cd-4328-a631-964b900a2897" />
Squeeze是压缩，Unsqueeze是增加维度  

2. 提出混合网络模型 LetNet，用于语义分割。使用简洁的解码编码结构，并使用transformer 作为胶囊网络去学习全局信息。同时，在跳跃连接中引入 特征增强模块（Feature Enhancement，FM），在恢复分辨率的过程中补充边界细节信息。  <img width="946" height="306" alt="image" src="https://github.com/user-attachments/assets/8fb5cc63-fc71-434c-8e5b-6c9047a575f3" />
编码器和解码器为 CNN 结构，用于提取局部特征，从而更好地表示图像。  Transformer 通过自注意力（Self-Attention）和多层感知机（MLP）结构，能够捕捉复杂的空间变换和远距离特征依赖，从而获得全局特征表示。  三条长跳跃连接的设计灵感来自 UNet ，它将低层次的空间信息与高层次的语义信息结合，从而实现高质量的分割效果。  

3. 在单张 RTX 3090 硬件平台上，LETNet 在 Cityscapes 测试集上取得了 72.8% 的 mIoU，仅使用 0.95M 参数量；在 CamVid 数据集上取得了 70.5% 的优异性能。该性能优于大多数现有模型。

论文及github：https://guangweigao.github.io/paper/TITS-LETNet.pdf  
https://github.com/IVIPLab/LETNet


## 常用激活函数和架构使用组合
如果没有激活函数，神经网络无论有多少层，都只能表示线性变换（多个线性层的叠加仍是线性的），无法解决非线性问题（如异或分类、图像识别等）。
激活函数（如ReLU、Sigmoid）可以对输入进行非线性变换，使神经网络可以拟合任意复杂的函数。没有非线性，神经网络就失去了解决复杂问题的能力。  
- CNN中ReLU能帮助模型捕捉图像的层次化特征。

## 模型训练
- 将模型分为训练集、验证集和测试集。常见比例为6：2：2，若训练集不多，可忽略测试集，训练集：验证集 = 8：2
- 设置训练超参数,如学习率、epoch数等。常用的有：
  - **学习率(Learning Rate, lr)**：每次参数更新时，沿着梯度下降的步长大小。太大可能导致训练不稳定，训练结果发散。太小收敛很慢，且可能会导致局部最优
    - 小Tips：可以使用学习率衰减，随着训练学习率不断减小
  - **批大小 (Batch Size)**：一次迭代中送入模型的样本数。大批量 (如 256, 512)：收敛稳定、利用 GPU 并行更高效，但显存消耗大，可能收敛到较差的点。小批量 (如 16, 32)：泛化能力可能更好，但训练更慢
    - 比如我们在训练 Yolov8 时使用的就是128 batch
  - **轮数 / 迭代次数 (Epochs)**：整个训练集被完整送入模型一次，称为一个 epoch。一般在50~200之间
    - epoch 数不是越多越好，太多可能过拟合
  - **优化器 (Optimizer)**：决定参数如何更新
    - SGD：经典、稳定，但可能收敛慢。Adam：自适应学习率，用得最广泛。RMSProp / AdamW：在深度学习里也很常见
  - **权重衰减 (Weight Decay / L2 正则化)**：通过惩罚大权重，防止过拟合
    - 在 AdamW、SGD 中常用，值一般在 1e-5 ~ 1e-3
  - **Dropout 比例**：在训练时随机“丢弃”部分神经元，增强模型的泛化能力
    - 一般在 0.1 ~ 0.5 之间
  - **动量 (Momentum)**：给参数更新加“惯性”，避免震荡


## 参考内容
https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%B8%89%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md  

https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/%E7%AC%AC%E4%BA%94%E7%AB%A0_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN).md  

LetNet 论文：  https://guangweigao.github.io/paper/TITS-LETNet.pdf  
https://github.com/IVIPLab/LETNet

前向传播、反向传播：  https://blog.csdn.net/qq_16137569/article/details/81449209?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522894da9ce6b4863afb52b2f798168c10f%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=894da9ce6b4863afb52b2f798168c10f&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-81449209-null-null.142^v102^pc_search_result_base1&utm_term=%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&spm=1018.2226.3001.4187  
http://neuralnetworksanddeeplearning.com/  

卷积神经网络CNN的反向传播原理：  
https://blog.csdn.net/qq_16137569/article/details/81477906?spm=1001.2014.3001.5502

